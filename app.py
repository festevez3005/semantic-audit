import streamlit as st
import numpy as np
import requests
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA 
import matplotlib.pyplot as plt
from typing import List

# --- Helper Functions and Model Loading ---

@st.cache_resource 
def load_model():
    """Carga el modelo SBERT multiling√ºe."""
    st.write("Cargando modelo sentence-transformer multiling√ºe...")
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

def fetch_content_from_url(url: str) -> str:
    """Extrae y limpia el contenido de texto de una URL."""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status() 
        soup = BeautifulSoup(response.content, 'html.parser')
        text_parts = soup.find_all(['p', 'h1', 'h2', 'h3', 'li'])
        content = ' '.join([part.get_text() for part in text_parts])
        clean_content = ' '.join(content.split()) 
        return clean_content[:4096] 
    except requests.exceptions.RequestException as e:
        return f"Error al obtener la URL: {e}"
    except Exception as e:
        return f"Ocurri√≥ un error inesperado: {e}"

def chunk_document(text: str, max_chunk_length: int = 500) -> List[str]:
    """Divide el texto del documento en p√°rrafos (chunks) basados en saltos de l√≠nea."""
    chunks = []
    current_chunk = ""
    
    # Intenta dividir por doble salto de l√≠nea (p√°rrafos)
    paragraphs = text.split('\n\n')
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph:
            continue
            
        # Si el p√°rrafo actual excede el l√≠mite, fuerza el cierre del chunk
        if len(current_chunk) + len(paragraph) > max_chunk_length and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = paragraph
        else:
            current_chunk += " " + paragraph
            
    if current_chunk:
        chunks.append(current_chunk.strip())
        
    # Si el chunking basado en p√°rrafos falla y solo queda un gran chunk, usa un divisor de caracteres simple
    if len(chunks) == 1 and len(chunks[0]) > 800:
        # Intenta un m√©todo m√°s simple de divisi√≥n forzada (por si el texto no tiene saltos de l√≠nea)
        chunks = [text[i:i + max_chunk_length] for i in range(0, len(text), max_chunk_length)]

    return [c.replace('\n', ' ') for c in chunks if c.strip()]


# --- Streamlit Application Layout ---

st.title("üåê Herramienta Multiling√ºe de Ingenier√≠a de Relevancia Sem√°ntica")
st.markdown("Mide la **alineaci√≥n sem√°ntica** de tu contenido y consulta.")

# Cargar el modelo
model = load_model()

# 1. √Årea de Entrada de Usuario
st.header("1. Definir Contenido y Consulta (English / Espa√±ol)")

source_type = st.radio(
    "Fuente del Contenido:",
    ('URL', 'Raw Text'),
    horizontal=True,
    key='source_radio'
)

document_text = ""
if source_type == 'URL':
    # --- VALOR ACTUALIZADO AQU√ç ---
    content_input = st.text_input("Ingresar URL del Documento:", "https://www.argentina.gob.ar/justicia/convosenlaweb/situaciones/que-es-la-inteligencia-artificial")
    # -----------------------------
    if content_input:
        with st.spinner('Obteniendo y limpiando contenido...'):
            document_text = fetch_content_from_url(content_input)
            if not document_text.startswith("Error"):
                st.success(f"Contenido obtenido y limpiado ({len(document_text)} caracteres).")
            else:
                st.error(document_text)
                document_text = ""
elif source_type == 'Raw Text':
    document_text = st.text_area("Pegar Texto del Documento:", height=200, 
                                 value="La inteligencia artificial (IA) es un campo de la inform√°tica dedicado a la resoluci√≥n de problemas cognitivos com√∫nmente asociados con la inteligencia humana.")

# --- VALOR ACTUALIZADO AQU√ç ---
query = st.text_input("Ingresar Consulta Objetivo:", "inteligencia artificial")
# -----------------------------

# 2. Bot√≥n de An√°lisis
if st.button("Calcular Relevancia Sem√°ntica", type="primary") and document_text and query:
    
    if document_text and not document_text.startswith("Error"):
        
        # ----------------------------------------------------
        # L√ìGICA: CHUNKING Y B√öSQUEDA DEL MEJOR MATCH
        # ----------------------------------------------------
        chunks = chunk_document(document_text)
        if not chunks:
            st.error("No se pudo segmentar el documento en partes utilizables. Intenta con un texto m√°s largo.")
            st.stop()
            
        st.info(f"Documento segmentado en **{len(chunks)}** partes para encontrar el mejor alineamiento sem√°ntico.")

        # 3. Generar Embeddings para todos los chunks y la query
        texts_to_embed = chunks + [query]
        with st.spinner('Generando vectores sem√°nticos para todos los segmentos...'):
            embeddings_full = model.encode(texts_to_embed)

        query_embedding = embeddings_full[-1].reshape(1, -1)
        chunk_embeddings = embeddings_full[:-1] 

        # 4. Calcular Similitud y encontrar el score m√°ximo
        similarity_scores = cosine_similarity(query_embedding, chunk_embeddings)[0]
        max_similarity_score = np.max(similarity_scores)
        best_chunk_index = np.argmax(similarity_scores)
        best_chunk = chunks[best_chunk_index]

        st.header("2. Resultado del An√°lisis Sem√°ntico")
        
        # ----------------------------------------------------
        # VISUALIZACI√ìN DE LA RELEVANCIA
        # ----------------------------------------------------
        
        st.metric(label="Puntuaci√≥n de Relevancia M√ÅXIMA (Similitud del Coseno)", value=f"{max_similarity_score:.4f}")
        
        st.subheader("Interpretaci√≥n de Ingenier√≠a de Relevancia")
        
        if max_similarity_score >= 0.7:
            st.success("‚úÖ **Excelente Alineaci√≥n.** El significado de al menos un segmento del contenido es altamente relevante para la intenci√≥n de la consulta.")
        elif max_similarity_score >= 0.4:
            st.warning("‚ö†Ô∏è **Alineaci√≥n Moderada.** Los temas est√°n relacionados, pero el contenido podr√≠a optimizarse para mejorar el alineamiento.")
        else:
            st.error("‚ùå **Poca Relevancia.** El contenido est√° sem√°nticamente distante de la consulta. Se necesita m√°s contexto relevante.")

        # ----------------------------------------------------
        # 5. GR√ÅFICO DE DISTANCIA VECTORIAL (X e Y)
        # ----------------------------------------------------
        st.subheader("Visualizaci√≥n del Espacio Sem√°ntico (PCA)")
        st.markdown(f"**Comparaci√≥n:** El **Mejor Segmento** ({len(best_chunk)} caracteres) vs. la **Consulta**.")
        
        # Generar embeddings SOLO para el mejor par para la visualizaci√≥n limpia
        embeddings_2d_pair = model.encode([best_chunk, query])

        # Aplicar PCA para reducir la dimensionalidad
        pca = PCA(n_components=2)
        vectors_2d = pca.fit_transform(embeddings_2d_pair)

        # Crear el gr√°fico
        fig, ax = plt.subplots()
        
        # Graficar los puntos
        ax.scatter(vectors_2d[0, 0], vectors_2d[0, 1], label='Mejor Segmento', color='blue', s=150)
        ax.scatter(vectors_2d[1, 0], vectors_2d[1, 1], label='Consulta (Intenci√≥n)', color='red', s=150)
        
        # Dibujar l√≠nea
        ax.plot([vectors_2d[0, 0], vectors_2d[1, 0]], [vectors_2d[0, 1], vectors_2d[1, 1]], 
                linestyle='--', color='gray', linewidth=1)
        
        # Etiquetas
        ax.annotate("Segmento", (vectors_2d[0, 0], vectors_2d[0, 1]), textcoords="offset points", xytext=(5,5), ha='center', color='blue')
        ax.annotate("Consulta", (vectors_2d[1, 0], vectors_2d[1, 1]), textcoords="offset points", xytext=(5,5), ha='center', color='red')
        
        ax.set_title(f'Distancia Vectorial (Score M√°ximo: {max_similarity_score:.4f})')
        ax.legend()
        ax.grid(True)
        
        st.pyplot(fig)
        
        # Mostrar el segmento m√°s relevante
        with st.expander("Ver Segmento del Documento M√°s Relevante"):
            st.markdown(f"**P√°rrafo (Chunk) con el Score M√°ximo ({max_similarity_score:.4f}):**")
            st.code(best_chunk, language='markdown')

    else:
        st.error("Aseg√∫rate de tener contenido de documento v√°lido y una consulta antes de ejecutar el an√°lisis.")

# ----------------------------------------------------
# 6. PIE DE P√ÅGINA Y CONTACTO
# ----------------------------------------------------
st.markdown("---") 
st.markdown("""
### Informaci√≥n y Contacto ü§ù
‚ú® Esta herramienta fue creada con **fines educativos y de asistencia a profesionales**.

üíå **¬øTe sirvi√≥? ¬øTen√©s alguna sugerencia? ¬øQuer√©s charlar sobre SEO, comunicaci√≥n digital o IA aplicada?** Escribinos a: **`hola@crawla.agency`**

üåê Conect√° con Crawla en **[LinkedIn](https://www.linkedin.com/company/crawla-agency/)**
""")
